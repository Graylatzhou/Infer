import os
import sys
import json
import torch
import torchaudio
import librosa
import numpy as np
import traceback
import re
from typing import Optional, List, Dict, Any, Tuple
from transformers import AutoModelForMaskedLM, AutoTokenizer
from time import time as ttime

sys.path.insert(0, "/2023022031/Infer/third_party/GPT_SoVITS/")

from GPT_SoVITS.feature_extractor import cnhubert
from GPT_SoVITS.AR.models.t2s_lightning_module import Text2SemanticLightningModule
from GPT_SoVITS.module.models import SynthesizerTrn, SynthesizerTrnV3, Generator
from GPT_SoVITS.text import cleaned_text_to_sequence
from GPT_SoVITS.text.cleaner import clean_text
from GPT_SoVITS.text.LangSegmenter import LangSegmenter
from GPT_SoVITS.module.mel_processing import mel_spectrogram_torch, spectrogram_torch
from GPT_SoVITS.process_ckpt import get_sovits_version_from_path_fast, load_sovits_new

# å°è¯•å¯¼å…¥å·¥å…·æ¨¡å—
try:
    from tools.assets import css, js, top_html
    from tools.i18n.i18n import I18nAuto, scan_language_list
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ç®€åŒ–ç‰ˆæœ¬
    def scan_language_list():
        return ["Auto", "zh", "en", "ja", "ko"]
    
    class I18nAuto:
        def __init__(self, language="zh"):
            self.language = language
        def __call__(self, text):
            return text

language = os.environ.get("language", "Auto")
i18n = I18nAuto(language=language)

device = "cuda" if torch.cuda.is_available() else "cpu"
is_half = True and torch.cuda.is_available()
dtype = torch.float16 if is_half else torch.float32

dict_language_v1 = {
    "ä¸­æ–‡": "all_zh",
    "è‹±æ–‡": "en", 
    "æ—¥æ–‡": "all_ja",
    "ä¸­è‹±æ··åˆ": "zh",
    "æ—¥è‹±æ··åˆ": "ja",
    "å¤šè¯­ç§æ··åˆ": "auto",
}

dict_language_v2 = {
    "ä¸­æ–‡": "all_zh",
    "è‹±æ–‡": "en",
    "æ—¥æ–‡": "all_ja", 
    "ç²¤è¯­": "all_yue",
    "éŸ©æ–‡": "all_ko",
    "ä¸­è‹±æ··åˆ": "zh",
    "æ—¥è‹±æ··åˆ": "ja",
    "ç²¤è‹±æ··åˆ": "yue",
    "éŸ©è‹±æ··åˆ": "ko",
    "å¤šè¯­ç§æ··åˆ": "auto",
    "å¤šè¯­ç§æ··åˆ(ç²¤è¯­)": "auto_yue",
}

base_dir = "/2023022031/Infer/"
bert_path = os.path.join(base_dir, "Bert")
cnhubert_model_path = os.path.join(base_dir, "hubert")
gpt_path = os.path.join(base_dir, "s1v3/s1v3.ckpt")
sovits_path = os.path.join(base_dir, "s2Gv3/s2Gv3.pth")
vocoder_path = os.path.join(base_dir, "bigvgan/")

cache = {}
splits = {"ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", ",", ".", "?", "!", "~", ":", "ï¼š", "â€”", "â€¦"}
punctuation = set(["!", "?", "â€¦", ",", ".", "-", " "])
resample_transform_dict = {}

tokenizer = AutoTokenizer.from_pretrained(bert_path)
bert_model = AutoModelForMaskedLM.from_pretrained(bert_path)
if is_half:
    bert_model = bert_model.half().to(device)
else:
    bert_model = bert_model.to(device)

cnhubert.cnhubert_base_path = cnhubert_model_path
ssl_model = cnhubert.get_model()
if is_half:
    ssl_model = ssl_model.half().to(device)
else:
    ssl_model = ssl_model.to(device)

class DictToAttrRecursive(dict):
    def __init__(self, input_dict):
        super().__init__(input_dict)
        for key, value in input_dict.items():
            if isinstance(value, dict):
                value = DictToAttrRecursive(value)
            self[key] = value
            setattr(self, key, value)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

    def __setattr__(self, key, value):
        if isinstance(value, dict):
            value = DictToAttrRecursive(value)
        super(DictToAttrRecursive, self).__setitem__(key, value)
        super().__setattr__(key, value)

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

def get_bert_feature(text, word2ph):
    """è·å–BERTç‰¹å¾"""
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt")
        for i in inputs:
            inputs[i] = inputs[i].to(device)
        res = bert_model(**inputs, output_hidden_states=True)
        res = torch.cat(res["hidden_states"][-3:-2], -1)[0].cpu()[1:-1]
    
    assert len(word2ph) == len(text)
    phone_level_feature = []
    for i in range(len(word2ph)):
        repeat_feature = res[i].repeat(word2ph[i], 1)
        phone_level_feature.append(repeat_feature)
    phone_level_feature = torch.cat(phone_level_feature, dim=0)
    return phone_level_feature.T

def resample(audio_tensor, sr0, sr1, device):
    """éŸ³é¢‘é‡é‡‡æ ·"""
    global resample_transform_dict
    key = f"{sr0}-{sr1}-{str(device)}"
    if key not in resample_transform_dict:
        resample_transform_dict[key] = torchaudio.transforms.Resample(sr0, sr1).to(device)
    return resample_transform_dict[key](audio_tensor)

def get_spepc(hps, filename, dtype, device, is_v2pro=False):
    """è·å–é¢‘è°±ç‰¹å¾"""
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    if sr0 != sr1:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, device)
    else:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)

    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    if is_v2pro:
        audio = resample(audio, sr1, 16000, device).to(dtype)
    return spec, audio

def clean_text_inf(text, language, version):
    """æ¸…ç†æ–‡æœ¬å¹¶è½¬æ¢ä¸ºéŸ³ç´ """
    language = language.replace("all_", "")
    phones, word2ph, norm_text = clean_text(text, language, version)
    phones = cleaned_text_to_sequence(phones, version)
    return phones, word2ph, norm_text

def get_bert_inf(phones, word2ph, norm_text, language):
    """è·å–BERTæ¨ç†ç‰¹å¾"""
    language = language.replace("all_", "")
    if language == "zh":
        bert = get_bert_feature(norm_text, word2ph).to(device)
    else:
        bert = torch.zeros(
            (1024, len(phones)),
            dtype=dtype,
        ).to(device)
    return bert

def get_phones_and_bert(text, language, version, final=False):
    """è·å–éŸ³ç´ å’ŒBERTç‰¹å¾"""
    text = re.sub(r' {2,}', ' ', text)
    textlist = []
    langlist = []
    
    # è¯­è¨€åˆ†å‰²
    if language == "all_zh":
        for tmp in LangSegmenter.getTexts(text, "zh"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_yue":
        for tmp in LangSegmenter.getTexts(text, "zh"):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ja":
        for tmp in LangSegmenter.getTexts(text, "ja"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ko":
        for tmp in LangSegmenter.getTexts(text, "ko"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "en":
        langlist.append("en")
        textlist.append(text)
    elif language == "auto":
        for tmp in LangSegmenter.getTexts(text):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "auto_yue":
        for tmp in LangSegmenter.getTexts(text):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    else:
        for tmp in LangSegmenter.getTexts(text):
            if langlist:
                if (tmp["lang"] == "en" and langlist[-1] == "en") or (tmp["lang"] != "en" and langlist[-1] != "en"):
                    textlist[-1] += tmp["text"]
                    continue
            if tmp["lang"] == "en":
                langlist.append(tmp["lang"])
            else:
                langlist.append(language)
            textlist.append(tmp["text"])
    
    print(f"æ–‡æœ¬åˆ‡åˆ†: {textlist}")
    print(f"è¯­è¨€è¯†åˆ«: {langlist}")
    
    phones_list = []
    bert_list = []
    norm_text_list = []
    
    for i in range(len(textlist)):
        lang = langlist[i]
        phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)
        bert = get_bert_inf(phones, word2ph, norm_text, lang)
        phones_list.append(phones)
        norm_text_list.append(norm_text)
        bert_list.append(bert)
    
    bert = torch.cat(bert_list, dim=1)
    phones = sum(phones_list, [])
    norm_text = "".join(norm_text_list)
    
    if not final and len(phones) < 6:
        return get_phones_and_bert("." + text, language, version, final=True)
    
    return phones, bert.to(dtype), norm_text

def merge_short_text_in_array(texts, threshold):
    """åˆå¹¶çŸ­æ–‡æœ¬"""
    if len(texts) < 2:
        return texts
    result = []
    text = ""
    for ele in texts:
        text += ele
        if len(text) >= threshold:
            result.append(text)
            text = ""
    if len(text) > 0:
        if len(result) == 0:
            result.append(text)
        else:
            result[len(result) - 1] += text
    return result

def process_text(texts):
    """å¤„ç†æ–‡æœ¬åˆ—è¡¨"""
    _text = []
    if all(text in [None, " ", "\n", ""] for text in texts):
        raise ValueError("è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
    for text in texts:
        if text and text.strip():
            _text.append(text)
    return _text

def split(todo_text):
    """åˆ†å‰²æ–‡æœ¬"""
    todo_text = todo_text.replace("â€¦â€¦", "ã€‚").replace("â€”â€”", "ï¼Œ")
    if todo_text[-1] not in splits:
        todo_text += "ã€‚"
    i_split_head = i_split_tail = 0
    len_text = len(todo_text)
    todo_texts = []
    while 1:
        if i_split_head >= len_text:
            break
        if todo_text[i_split_head] in splits:
            i_split_head += 1
            todo_texts.append(todo_text[i_split_tail:i_split_head])
            i_split_tail = i_split_head
        else:
            i_split_head += 1
    return todo_texts

def cut1(inp):
    """å‡‘å››å¥ä¸€åˆ‡"""
    inp = inp.strip("\n")
    inps = split(inp)
    split_idx = list(range(0, len(inps), 4))
    split_idx[-1] = None
    if len(split_idx) > 1:
        opts = []
        for idx in range(len(split_idx) - 1):
            opts.append("".join(inps[split_idx[idx] : split_idx[idx + 1]]))
    else:
        opts = [inp]
    opts = [item for item in opts if not set(item).issubset(punctuation)]
    return "\n".join(opts)

def cut2(inp):
    """å‡‘50å­—ä¸€åˆ‡"""
    inp = inp.strip("\n")
    inps = split(inp)
    if len(inps) < 2:
        return inp
    opts = []
    summ = 0
    tmp_str = ""
    for i in range(len(inps)):
        summ += len(inps[i])
        tmp_str += inps[i]
        if summ > 50:
            summ = 0
            opts.append(tmp_str)
            tmp_str = ""
    if tmp_str != "":
        opts.append(tmp_str)
    if len(opts) > 1 and len(opts[-1]) < 50:
        opts[-2] = opts[-2] + opts[-1]
        opts = opts[:-1]
    opts = [item for item in opts if not set(item).issubset(punctuation)]
    return "\n".join(opts)

def cut3(inp):
    """æŒ‰ä¸­æ–‡å¥å·åˆ‡"""
    inp = inp.strip("\n")
    opts = ["%s" % item for item in inp.strip("ã€‚").split("ã€‚")]
    opts = [item for item in opts if not set(item).issubset(punctuation)]
    return "\n".join(opts)

def cut4(inp):
    """æŒ‰è‹±æ–‡å¥å·åˆ‡"""
    inp = inp.strip("\n")
    opts = re.split(r"(?<!\d)\.(?!\d)", inp.strip("."))
    opts = [item for item in opts if not set(item).issubset(punctuation)]
    return "\n".join(opts)

def cut5(inp):
    """æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"""
    inp = inp.strip("\n")
    punds = {",", ".", ";", "?", "!", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", ";", "ï¼š", "â€¦"}
    mergeitems = []
    items = []
    
    for i, char in enumerate(inp):
        if char in punds:
            if char == "." and i > 0 and i < len(inp) - 1 and inp[i - 1].isdigit() and inp[i + 1].isdigit():
                items.append(char)
            else:
                items.append(char)
                mergeitems.append("".join(items))
                items = []
        else:
            items.append(char)
    
    if items:
        mergeitems.append("".join(items))
    
    opt = [item for item in mergeitems if not set(item).issubset(punds)]
    return "\n".join(opt)

spec_min = -12
spec_max = 2

def norm_spec(x):
    return (x - spec_min) / (spec_max - spec_min) * 2 - 1

def denorm_spec(x):
    return (x + 1) / 2 * (spec_max - spec_min) + spec_min

mel_fn = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1024,
        "win_size": 1024,
        "hop_size": 256,
        "num_mels": 100,
        "sampling_rate": 24000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)

mel_fn_v4 = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1280,
        "win_size": 1280,
        "hop_size": 320,
        "num_mels": 100,
        "sampling_rate": 32000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)
bigvgan_model = None

def clean_bigvgan_model():
    global bigvgan_model
    if bigvgan_model:
        bigvgan_model = bigvgan_model.cpu()
        bigvgan_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass

def init_bigvgan():
    global bigvgan_model
    try:
        from third_party.GPT_SoVITS.GPT_SoVITS.BigVGAN import bigvgan
        
        bigvgan_model = bigvgan.BigVGAN.from_pretrained(
            vocoder_path,
            use_cuda_kernel=False,
        )
        bigvgan_model.remove_weight_norm()
        bigvgan_model = bigvgan_model.eval()
        
        if is_half:
            bigvgan_model = bigvgan_model.half().to(device)
        else:
            bigvgan_model = bigvgan_model.to(device)

    except Exception as e:
        print(f"âŒ BigVGAN æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        bigvgan_model = None

# ===== ä¸»è¦æ¨¡å‹åŠ è½½å‡½æ•° =====
def load_gpt_model(gpt_path):
    dict_s1 = torch.load(gpt_path, map_location="cpu", weights_only=False)
    config = dict_s1["config"]
    max_sec = config["data"]["max_sec"]
    hz = 50
    
    t2s_model = Text2SemanticLightningModule(config, "****", is_train=False)
    t2s_model.load_state_dict(dict_s1["weight"])
    if is_half:
        t2s_model = t2s_model.half()
    t2s_model = t2s_model.to(device)
    t2s_model.eval()
    
    return t2s_model, config, max_sec, hz

def load_sovits_model(sovits_path):
    version, model_version, if_lora_v3 = get_sovits_version_from_path_fast(sovits_path)
    dict_s2 = load_sovits_new(sovits_path)
    hps = dict_s2["config"]
    hps = DictToAttrRecursive(hps)
    hps.model.semantic_frame_rate = "25hz"
    
    if "enc_p.text_embedding.weight" not in dict_s2["weight"]:
        hps.model.version = "v2"
    elif dict_s2["weight"]["enc_p.text_embedding.weight"].shape[0] == 322:
        hps.model.version = "v1"
    else:
        hps.model.version = "v2"
    
    version = hps.model.version
    
    # åˆ›å»ºæ¨¡å‹
    v3v4set = {"v3", "v4"}
    if model_version not in v3v4set:
        vq_model = SynthesizerTrn(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **hps.model,
        )
    else:
        hps.model.version = model_version
        vq_model = SynthesizerTrnV3(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **hps.model,
        )
    
    if "pretrained" not in sovits_path:
        try:
            del vq_model.enc_q
        except:
            pass
    
    if is_half:
        vq_model = vq_model.half().to(device)
    else:
        vq_model = vq_model.to(device)
    vq_model.eval()
    
    vq_model.load_state_dict(dict_s2["weight"], strict=False)
    
    return vq_model, hps, version, model_version

def get_tts_wav(
    ref_wav_path: str,
    prompt_text: str,
    prompt_language: str,
    text: str,
    text_language: str,
    how_to_cut: str = "ä¸åˆ‡",
    top_k: int = 20,
    top_p: float = 0.6,
    temperature: float = 0.6,
    ref_free: bool = False,
    speed: float = 1.0,
    if_freeze: bool = False,
    sample_steps: int = 32,
    pause_second: float = 0.3,
):
    """æ ¸å¿ƒTTSæ¨ç†å‡½æ•°"""
    global cache
    
    # è¾“å…¥éªŒè¯
    if not ref_wav_path or not os.path.exists(ref_wav_path):
        raise ValueError("è¯·æä¾›æœ‰æ•ˆçš„å‚è€ƒéŸ³é¢‘è·¯å¾„")
    if not text.strip():
        raise ValueError("è¯·å¡«å…¥æ¨ç†æ–‡æœ¬")
    
    print(f"ğŸ¯ å¼€å§‹æ¨ç†...")
    print(f"å‚è€ƒéŸ³é¢‘: {ref_wav_path}")
    print(f"å‚è€ƒæ–‡æœ¬: {prompt_text}")
    print(f"ç›®æ ‡æ–‡æœ¬: {text}")
    
    t = []
    t0 = ttime()
    
    # è¯­è¨€è½¬æ¢
    dict_language = dict_language_v1 if version == "v1" else dict_language_v2
    prompt_language = dict_language[prompt_language]
    text_language = dict_language[text_language]
    
    # åˆ¤æ–­æ˜¯å¦æ— å‚è€ƒæ–‡æœ¬æ¨¡å¼
    if not prompt_text or len(prompt_text) == 0:
        ref_free = True
    
    # v3æš‚ä¸æ”¯æŒref_free
    if model_version in {"v3", "v4"}:
        ref_free = False
    
    # å¤„ç†å‚è€ƒæ–‡æœ¬
    if not ref_free:
        prompt_text = prompt_text.strip("\n")
        if prompt_text and prompt_text[-1] not in splits:
            prompt_text += "ã€‚" if prompt_language != "en" else "."
    
    # å¤„ç†ç›®æ ‡æ–‡æœ¬
    text = text.strip("\n")
    # åˆ›å»ºé™éŸ³ç‰‡æ®µ
    zero_wav = np.zeros(
        int(hps.data.sampling_rate * pause_second),
        dtype=np.float16 if is_half else np.float32,
    )
    zero_wav_torch = torch.from_numpy(zero_wav)
    if is_half:
        zero_wav_torch = zero_wav_torch.half().to(device)
    else:
        zero_wav_torch = zero_wav_torch.to(device)
    
    # å¤„ç†å‚è€ƒéŸ³é¢‘
    if not ref_free:
        with torch.no_grad():
            wav16k, sr = librosa.load(ref_wav_path, sr=16000)
            if wav16k.shape[0] > 160000 or wav16k.shape[0] < 48000:
                raise ValueError("å‚è€ƒéŸ³é¢‘é•¿åº¦åº”åœ¨3-10ç§’ä¹‹é—´")
            
            wav16k = torch.from_numpy(wav16k)
            if is_half:
                wav16k = wav16k.half().to(device)
            else:
                wav16k = wav16k.to(device)
            wav16k = torch.cat([wav16k, zero_wav_torch])
            
            # æå–SSLç‰¹å¾
            ssl_content = ssl_model.model(wav16k.unsqueeze(0))["last_hidden_state"].transpose(1, 2)
            codes = vq_model.extract_latent(ssl_content)
            prompt_semantic = codes[0, 0]
            prompt = prompt_semantic.unsqueeze(0).to(device)
    
    t1 = ttime()
    t.append(t1 - t0)
    
    # æ–‡æœ¬åˆ‡åˆ†
    if how_to_cut == "å‡‘å››å¥ä¸€åˆ‡":
        text = cut1(text)
    elif how_to_cut == "å‡‘50å­—ä¸€åˆ‡":
        text = cut2(text)
    elif how_to_cut == "æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡":
        text = cut3(text)
    elif how_to_cut == "æŒ‰è‹±æ–‡å¥å·.åˆ‡":
        text = cut4(text)
    elif how_to_cut == "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡":
        text = cut5(text)
    
    while "\n\n" in text:
        text = text.replace("\n\n", "\n")
    
    texts = text.split("\n")
    texts = process_text(texts)
    texts = merge_short_text_in_array(texts, 5)
    audio_opt = []
    
    if not ref_free:
        phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)
    
    for i_text, text_segment in enumerate(texts):
        if len(text_segment.strip()) == 0:
            continue
            
        if text_segment[-1] not in splits:
            text_segment += "ã€‚" if text_language != "en" else "."
        
        phones2, bert2, norm_text2 = get_phones_and_bert(text_segment, text_language, version)
        
        # åˆå¹¶ç‰¹å¾
        if not ref_free:
            bert = torch.cat([bert1, bert2], 1)
            all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(device).unsqueeze(0)
        else:
            bert = bert2
            all_phoneme_ids = torch.LongTensor(phones2).to(device).unsqueeze(0)
        
        bert = bert.to(device).unsqueeze(0)
        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)
        
        t2 = ttime()
        
        # GPTæ¨ç† - ç”Ÿæˆè¯­ä¹‰token
        if i_text in cache and if_freeze:
            pred_semantic = cache[i_text]
            print(f"ä½¿ç”¨ç¼“å­˜çš„è¯­ä¹‰ç‰¹å¾")
        else:
            with torch.no_grad():
                pred_semantic, idx = t2s_model.model.infer_panel(
                    all_phoneme_ids,
                    all_phoneme_len,
                    None if ref_free else prompt,
                    bert,
                    top_k=top_k,
                    top_p=top_p,
                    temperature=temperature,
                    early_stop_num=hz * max_sec,
                )
                pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)
                cache[i_text] = pred_semantic
        
        t3 = ttime()
        
        # SoVITSè§£ç  - ç”ŸæˆéŸ³é¢‘
        v3v4set = {"v3", "v4"}
        if model_version not in v3v4set:
            # v1/v2ç‰ˆæœ¬çš„å¤„ç†
            refers, audio_tensor = get_spepc(hps, ref_wav_path, dtype, device)
            refers = [refers]
            
            audio = vq_model.decode(
                pred_semantic, 
                torch.LongTensor(phones2).to(device).unsqueeze(0), 
                refers, 
                speed=speed
            )[0][0]
        else:

            refer, audio_tensor = get_spepc(hps, ref_wav_path, dtype, device)
            phoneme_ids0 = torch.LongTensor(phones1).to(device).unsqueeze(0)
            phoneme_ids1 = torch.LongTensor(phones2).to(device).unsqueeze(0)
            fea_ref, ge = vq_model.decode_encp(prompt.unsqueeze(0), phoneme_ids0, refer)
            
            ref_audio, sr = torchaudio.load(ref_wav_path)
            ref_audio = ref_audio.to(device).float()
            if ref_audio.shape[0] == 2:
                ref_audio = ref_audio.mean(0).unsqueeze(0)
            
            tgt_sr = 24000 if model_version == "v3" else 32000
            if sr != tgt_sr:
                ref_audio = resample(ref_audio, sr, tgt_sr, device)
            
            mel2 = mel_fn(ref_audio) if model_version == "v3" else mel_fn_v4(ref_audio)
            mel2 = norm_spec(mel2)
            T_min = min(mel2.shape[2], fea_ref.shape[2])
            mel2 = mel2[:, :, :T_min]
            fea_ref = fea_ref[:, :, :T_min]
            
            Tref = 468 if model_version == "v3" else 500
            Tchunk = 934 if model_version == "v3" else 1000
            if T_min > Tref:
                mel2 = mel2[:, :, -Tref:]
                fea_ref = fea_ref[:, :, -Tref:]
                T_min = Tref
            
            chunk_len = Tchunk - T_min
            mel2 = mel2.to(dtype)
            fea_todo, ge = vq_model.decode_encp(pred_semantic, phoneme_ids1, refer, ge, speed)
            
            cfm_resss = []
            idx = 0
            while True:
                fea_todo_chunk = fea_todo[:, :, idx : idx + chunk_len]
                if fea_todo_chunk.shape[-1] == 0:
                    break
                idx += chunk_len
                fea = torch.cat([fea_ref, fea_todo_chunk], 2).transpose(2, 1)
                cfm_res = vq_model.cfm.inference(
                    fea, torch.LongTensor([fea.size(1)]).to(fea.device), mel2, sample_steps, inference_cfg_rate=0
                )
                cfm_res = cfm_res[:, :, mel2.shape[2] :]
                mel2 = cfm_res[:, :, -T_min:]
                fea_ref = fea_todo_chunk[:, :, -T_min:]
                cfm_resss.append(cfm_res)
            
            cfm_res = torch.cat(cfm_resss, 2)
            cfm_res = denorm_spec(cfm_res)
            
            # ä½¿ç”¨BigVGANç”ŸæˆéŸ³é¢‘
            if bigvgan_model is None:
                init_bigvgan()
            
            if bigvgan_model is not None:
                with torch.inference_mode():
                    wav_gen = bigvgan_model(cfm_res)
                    audio = wav_gen[0][0]
            else:
                raise RuntimeError("BigVGANæ¨¡å‹æœªåŠ è½½æˆåŠŸ")
        
        # é˜²æ­¢çˆ†éŸ³
        max_audio = torch.abs(audio).max()
        if max_audio > 1:
            audio = audio / max_audio
        
        audio_opt.append(audio)
        audio_opt.append(zero_wav_torch)  # æ·»åŠ é™éŸ³é—´éš”
        
        t4 = ttime()
        t.extend([t2 - t1, t3 - t2, t4 - t3])
        t1 = ttime()
    
    # åˆå¹¶éŸ³é¢‘
    audio_opt = torch.cat(audio_opt, 0)
    
    # ç¡®å®šé‡‡æ ·ç‡
    if model_version in {"v1", "v2", "v2Pro", "v2ProPlus"}:
        opt_sr = 32000
    elif model_version == "v3":
        opt_sr = 24000
    else:
        opt_sr = 48000
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    audio_opt = audio_opt.cpu().detach().numpy()
    return opt_sr, (audio_opt * 32767).astype(np.int16)

# ===== ç®€åŒ–APIæ¥å£ =====
class GPTSoVITSInference:
    """GPT-SoVITSæ¨ç†ç±»"""
    
    def __init__(self, gpt_model_path=None, sovits_model_path=None):
        """åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        self.gpt_path = gpt_model_path or gpt_path
        self.sovits_path = sovits_model_path or sovits_path
        
        # åŠ è½½æ¨¡å‹
        self.t2s_model, self.config, self.max_sec, self.hz = load_gpt_model(self.gpt_path)
        self.vq_model, self.hps, self.version, self.model_version = load_sovits_model(self.sovits_path)
        
        # è®¾ç½®å…¨å±€å˜é‡
        global t2s_model, config, max_sec, hz, vq_model, hps, version, model_version
        t2s_model = self.t2s_model
        config = self.config
        max_sec = self.max_sec
        hz = self.hz
        vq_model = self.vq_model
        hps = self.hps
        version = self.version
        model_version = self.model_version
        
        # åˆå§‹åŒ–vocoder
        if self.model_version == "v3":
            init_bigvgan()
        
    
    def synthesize(
        self,
        ref_audio_path: str,
        ref_text: str,
        target_text: str,
        ref_language: str = "ä¸­æ–‡",
        target_language: str = "ä¸­æ–‡",
        how_to_cut: str = "å‡‘å››å¥ä¸€åˆ‡",
        top_k: int = 15,
        top_p: float = 1.0,
        temperature: float = 1.0,
        speed: float = 1.0,
        sample_steps: int = 32,
    ) -> Tuple[int, np.ndarray]:
        """
        è¯­éŸ³åˆæˆä¸»æ¥å£
        
        Args:
            ref_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            ref_text: å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬
            target_text: è¦åˆæˆçš„ç›®æ ‡æ–‡æœ¬
            ref_language: å‚è€ƒéŸ³é¢‘è¯­è¨€
            target_language: ç›®æ ‡æ–‡æœ¬è¯­è¨€
            how_to_cut: æ–‡æœ¬åˆ‡åˆ†æ–¹å¼
            top_k: GPTé‡‡æ ·å‚æ•°
            top_p: GPTé‡‡æ ·å‚æ•°
            temperature: GPTé‡‡æ ·å‚æ•°
            speed: è¯­é€Ÿè°ƒèŠ‚
            sample_steps: é‡‡æ ·æ­¥æ•°(ä»…v3/v4)
            
        Returns:
            (sample_rate, audio_data): é‡‡æ ·ç‡å’ŒéŸ³é¢‘æ•°æ®
        """
        return get_tts_wav(
            ref_wav_path=ref_audio_path,
            prompt_text=ref_text,
            prompt_language=ref_language,
            text=target_text,
            text_language=target_language,
            how_to_cut=how_to_cut,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            speed=speed,
            sample_steps=sample_steps,
        )

# ===== æµ‹è¯•å‡½æ•° =====
def test_inference(target_text=None):
    
    ref_audio_path = "/2023022031/Infer/1.wav"  # å‚è€ƒéŸ³é¢‘è·¯å¾„
    ref_text = "èƒ½ä»¥è¿™ç§æ–¹å¼è§åˆ°ä½ ï¼Œæˆ‘çœŸçš„å¥½å¹¸è¿ï¼Œä½ å¸¦ç»™äº†æˆ‘ï¼Œä¸€ç›´æ¸´æœ›å´åˆä¸æ•¢å¥¢æ±‚çš„ç¤¼ç‰©ã€‚"  # å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬
    if target_text is None:
        target_text = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­éŸ³åˆæˆçš„æµ‹è¯•ã€‚ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€‚"  # è¦åˆæˆçš„æ–‡æœ¬
    
    # æ£€æŸ¥å‚è€ƒéŸ³é¢‘æ˜¯å¦å­˜åœ¨
    if not os.path.exists(ref_audio_path):
        print(f"âŒ å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {ref_audio_path}")
        print("è¯·æä¾›æœ‰æ•ˆçš„å‚è€ƒéŸ³é¢‘æ–‡ä»¶")
        return
    
    try:
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        inference_engine = GPTSoVITSInference()
        
        # æ‰§è¡Œæ¨ç†
        sample_rate, audio_data = inference_engine.synthesize(
            ref_audio_path=ref_audio_path,
            ref_text=ref_text,
            target_text=target_text,
            ref_language="ä¸­æ–‡",
            target_language="ä¸­æ–‡",
            how_to_cut="å‡‘å››å¥ä¸€åˆ‡",
            top_k=15,
            top_p=1.0,
            temperature=1.0,
            speed=1.0,
            sample_steps=32
        )
        
        print(f"âœ… æ¨ç†å®Œæˆ!")
        print(f"é‡‡æ ·ç‡: {sample_rate}")
        print(f"éŸ³é¢‘é•¿åº¦: {len(audio_data)} samples")
        print(f"éŸ³é¢‘æ—¶é•¿: {len(audio_data) / sample_rate:.2f}s")
        
        # ä¿å­˜éŸ³é¢‘
        output_path = "/2023022031/Infer/output_test.wav"
        import soundfile as sf
        sf.write(output_path, audio_data, sample_rate)
        print(f"éŸ³é¢‘å·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        traceback.print_exc()

def main():
    """ä¸»ç¨‹åº"""
    model_path = "/2023022031/Infer/Qwen"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    # load_custom_qwen
    llm = LLMEngine(
		model=model_path
	)
    samplingParams = SamplingParams(
		temperature=0.6,
		max_tokens=8192,
	)
    prompts = [
        "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
    ]
    prompts = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True
        )
        for prompt in prompts
    ]
    outputs = llm.generate(prompts, sampling_params=samplingParams)
    for prompt, output in zip(prompts, outputs):
        print("\n")
        print(f"Prompt: {prompt}")
        print(f"Completion: {output['text']!r}")
    text = output['text']
    text = re.sub(r'(<think>|<\|im_end\|>|</think>)', '', text)
    try:
        if not os.path.exists(gpt_path):
            print(f"âŒ GPTæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {gpt_path}")
            return
            
        if not os.path.exists(sovits_path):
            print(f"âŒ SoVITSæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {sovits_path}")
            return
        
        # è¿è¡Œæµ‹è¯•
        test_inference(text)
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        traceback.print_exc()

from infer.engine.llm_engine import LLMEngine, SamplingParams
from transformers import AutoTokenizer

if __name__ == "__main__":
    main()